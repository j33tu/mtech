{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba1d4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import plot_model,to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten,GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d439cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow_datasets\n",
      "  Obtaining dependency information for tensorflow_datasets from https://files.pythonhosted.org/packages/8f/50/52fa3d41d20c687d81f66338bc1b0e71a27a3390ecfa8f5bc212a10135e1/tensorflow_datasets-4.9.6-py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jitendra\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (8.0.4)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Obtaining dependency information for dm-tree from https://files.pythonhosted.org/packages/e4/c1/522041457444b67125ac9527208bb3148f63d7dce0a86ffa589ec763a10e/dm_tree-0.1.8-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Obtaining dependency information for immutabledict from https://files.pythonhosted.org/packages/e2/13/3cf4ac5b3403f3456e645c4533883ef67b1bb0c72e56b79c707715f57a74/immutabledict-4.2.0-py3-none-any.whl.metadata\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (1.24.3)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\jitendra\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow_datasets) (4.25.3)\n",
      "Requirement already satisfied: psutil in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Obtaining dependency information for simple-parsing from https://files.pythonhosted.org/packages/b0/12/c657047c11a47e1c3e51bdc26bd6f2661a268fd0384bd8ed56b227530486/simple_parsing-0.1.5-py3-none-any.whl.metadata\n",
      "  Downloading simple_parsing-0.1.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Obtaining dependency information for tensorflow-metadata from https://files.pythonhosted.org/packages/aa/47/91d1eea615b12473853edcbc0db02c94918e498bdf36049dde13bf24100d/tensorflow_metadata-1.15.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\jitendra\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Requirement already satisfied: toml in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (4.65.0)\n",
      "Requirement already satisfied: wrapt in c:\\program files\\conda\\lib\\site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Collecting etils[enp,epath,epy,etree]>=1.9.1 (from tensorflow_datasets)\n",
      "  Obtaining dependency information for etils[enp,epath,epy,etree]>=1.9.1 from https://files.pythonhosted.org/packages/a0/f4/305f3ea85aecd23422c606c179fb6d00bd7d255b10d55b4c797a3a680144/etils-1.9.2-py3-none-any.whl.metadata\n",
      "  Downloading etils-1.9.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: fsspec in c:\\program files\\conda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1->tensorflow_datasets) (2023.4.0)\n",
      "Collecting importlib_resources (from etils[enp,epath,epy,etree]>=1.9.1->tensorflow_datasets)\n",
      "  Obtaining dependency information for importlib_resources from https://files.pythonhosted.org/packages/75/06/4df55e1b7b112d183f65db9503bff189e97179b256e1ea450a3c365241e0/importlib_resources-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\program files\\conda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1->tensorflow_datasets) (4.7.1)\n",
      "Requirement already satisfied: zipp in c:\\program files\\conda\\lib\\site-packages (from etils[enp,epath,epy,etree]>=1.9.1->tensorflow_datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\conda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\conda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\conda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\conda\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\program files\\conda\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\program files\\conda\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting docstring-parser~=0.15 (from simple-parsing->tensorflow_datasets)\n",
      "  Obtaining dependency information for docstring-parser~=0.15 from https://files.pythonhosted.org/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl.metadata\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Obtaining dependency information for googleapis-common-protos<2,>=1.56.4 from https://files.pythonhosted.org/packages/02/48/87422ff1bddcae677fb6f58c97f5cfc613304a5e8ce2c3662760199c0a84/googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "   ---------------------------------------- 0.0/5.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/5.1 MB 1.7 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.2/5.1 MB 2.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.4/5.1 MB 2.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.6/5.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.0/5.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.0/5.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.6/5.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.3/5.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.3/5.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.7/5.1 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.1/5.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.5/5.1 MB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.9/5.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.1/5.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.1/5.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.1/5.1 MB 7.2 MB/s eta 0:00:00\n",
      "Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.3 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.3 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.3/101.3 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
      "   ---------------------------------------- 0.0/113.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  112.6/113.6 kB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 113.6/113.6 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/220.0 kB ? eta -:--:--\n",
      "   ---------------------------------------  215.0/220.0 kB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 220.0/220.0 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading etils-1.9.2-py3-none-any.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/161.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 161.5/161.5 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21548 sha256=3f2366c1001bf5e9c4a504f46dbf39259ddd7d92ea1bfbe770a8cbbe3c71dfb5\n",
      "  Stored in directory: c:\\users\\jitendra\\appdata\\local\\pip\\cache\\wheels\\90\\74\\b1\\9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, promise, importlib_resources, immutabledict, googleapis-common-protos, etils, docstring-parser, tensorflow-metadata, simple-parsing, tensorflow_datasets\n",
      "Successfully installed dm-tree-0.1.8 docstring-parser-0.16 etils-1.9.2 googleapis-common-protos-1.63.2 immutabledict-4.2.0 importlib_resources-6.4.0 promise-2.3 simple-parsing-0.1.5 tensorflow-metadata-1.15.0 tensorflow_datasets-4.9.6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tfds.exe is installed in 'C:\\Users\\jitendra\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2c9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06f7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ad1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (32, 32)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47aca8fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All values in column x_col=file_id must be strings.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      3\u001b[0m train_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[0;32m      5\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mtrain_df,\n\u001b[0;32m      6\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDevOps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmtech\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhackathon\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mIMAGE_SIZE,\n\u001b[0;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[0;32m     11\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1208\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_dataframe\u001b[1;34m(self, dataframe, directory, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, subset, interpolation, validate_filenames, **kwargs)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   1202\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_duplicates is deprecated, you can drop duplicates \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby using the pandas.DataFrame.drop_duplicates method.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1205\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m   1206\u001b[0m     )\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameIterator(\n\u001b[0;32m   1209\u001b[0m     dataframe,\n\u001b[0;32m   1210\u001b[0m     directory,\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1212\u001b[0m     x_col\u001b[38;5;241m=\u001b[39mx_col,\n\u001b[0;32m   1213\u001b[0m     y_col\u001b[38;5;241m=\u001b[39my_col,\n\u001b[0;32m   1214\u001b[0m     weight_col\u001b[38;5;241m=\u001b[39mweight_col,\n\u001b[0;32m   1215\u001b[0m     target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m   1216\u001b[0m     color_mode\u001b[38;5;241m=\u001b[39mcolor_mode,\n\u001b[0;32m   1217\u001b[0m     classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[0;32m   1218\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39mclass_mode,\n\u001b[0;32m   1219\u001b[0m     data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1220\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1221\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1222\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1223\u001b[0m     save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1224\u001b[0m     save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1225\u001b[0m     save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1226\u001b[0m     subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1227\u001b[0m     interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   1228\u001b[0m     validate_filenames\u001b[38;5;241m=\u001b[39mvalidate_filenames,\n\u001b[0;32m   1229\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1230\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:751\u001b[0m, in \u001b[0;36mDataFrameIterator.__init__\u001b[1;34m(self, dataframe, directory, image_data_generator, x_col, y_col, weight_col, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, subset, interpolation, keep_aspect_ratio, dtype, validate_filenames)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# check that inputs match the required class_mode\u001b[39;00m\n\u001b[1;32m--> 751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(df, x_col, y_col, weight_col, classes)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    753\u001b[0m     validate_filenames\n\u001b[0;32m    754\u001b[0m ):  \u001b[38;5;66;03m# check which image files are valid and keep them\u001b[39;00m\n\u001b[0;32m    755\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_valid_filepaths(df, x_col)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:813\u001b[0m, in \u001b[0;36mDataFrameIterator._check_params\u001b[1;34m(self, df, x_col, y_col, weight_col, classes)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# check that filenames/filepaths column values are all strings\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(df[x_col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m))):\n\u001b[1;32m--> 813\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll values in column x_col=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     )\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# check labels are string if class_mode is binary or sparse\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "\u001b[1;31mTypeError\u001b[0m: All values in column x_col=file_id must be strings."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f13e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'label': ['fake', 'real', 'fake', 'real'],\n",
    "    'file_id': ['img1.jpg', 'img2.jpg', 'img3.jpg', 'img4.jpg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bcf7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'label': ['fake', 'real', 'fake', 'real'],\n",
    "    'file_id': ['img1.jpg', 'img2.jpg', 'img3.jpg', 'img4.jpg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8472b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53bfb8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilep1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m],df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m'\u001b[39m]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#df['filep1']=df.apply(lambda row: os.path.join(df['label'], df['file_id']), axis=1)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32mC:\\Program Files\\conda\\Lib\\site-packages\\pandas\\core\\frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9422\u001b[0m )\n\u001b[1;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\conda\\Lib\\site-packages\\pandas\\core\\apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mC:\\Program Files\\conda\\Lib\\site-packages\\pandas\\core\\apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mC:\\Program Files\\conda\\Lib\\site-packages\\pandas\\core\\apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(v)\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilep1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m],df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m'\u001b[39m]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#df['filep1']=df.apply(lambda row: os.path.join(df['label'], df['file_id']), axis=1)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32m<frozen ntpath>:108\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Series"
     ]
    }
   ],
   "source": [
    "df['filep1']=df.apply(lambda row: os.path.join(df['label'],df['file_id']),axis=1)\n",
    "#df['filep1']=df.apply(lambda row: os.path.join(df['label'], df['file_id']), axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5011e403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>file_id</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>img1.jpg</td>\n",
       "      <td>fakeimg1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>real</td>\n",
       "      <td>img2.jpg</td>\n",
       "      <td>realimg2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>img3.jpg</td>\n",
       "      <td>fakeimg3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>real</td>\n",
       "      <td>img4.jpg</td>\n",
       "      <td>realimg4.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label   file_id     file_path\n",
       "0  fake  img1.jpg  fakeimg1.jpg\n",
       "1  real  img2.jpg  realimg2.jpg\n",
       "2  fake  img3.jpg  fakeimg3.jpg\n",
       "3  real  img4.jpg  realimg4.jpg"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "553f030b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label   file_id     file_path     file_path1\n",
      "0  fake  img1.jpg  fakeimg1.jpg  fake\\img1.jpg\n",
      "1  real  img2.jpg  realimg2.jpg  real\\img2.jpg\n",
      "2  fake  img3.jpg  fakeimg3.jpg  fake\\img3.jpg\n",
      "3  real  img4.jpg  realimg4.jpg  real\\img4.jpg\n"
     ]
    }
   ],
   "source": [
    "df['file_path1'] = df.apply(lambda row: os.path.join(row['label'], row['file_id']), axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
