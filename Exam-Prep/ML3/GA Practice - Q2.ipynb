{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7b2f61",
   "metadata": {},
   "source": [
    "# GA Practice - Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a787b93",
   "metadata": {},
   "source": [
    "We combined two datasets in single CSV file. Use the below script to seperate the data.\n",
    "\n",
    "Use data_dime for dimensionality reduction and data_recom for recommendation system.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('dataset3.csv')\n",
    "\n",
    "data_dime=data.iloc[0:5891,0:30]\n",
    "\n",
    "data_recom=data.iloc[:,30:34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88339447",
   "metadata": {},
   "source": [
    "## SECTION A - (5 marks)\n",
    "### 1. Data Preprocessing \n",
    "\n",
    "    a. Read the dataset and perform required cleaning and preprocessing prior to model building. (1 MARK)\n",
    "    \n",
    "    b. Calculate five-point summary for numerical variables. Summarize observations for categorical variables – no. of categories, % observations in each category. (1 MARK)\n",
    "    \n",
    "    c. Perform univariate and bivariate analysis (2 MARKS)\n",
    "    \n",
    "    d. Scale / Transform/ clean the data so that it is suitable for model building. Drop \"SalePrice\" before using clustering methods, as this is the target attribute. (1 MARK)\n",
    "\n",
    "## SECTION B (20 marks)\n",
    "### 2. Answer the following questions \n",
    "\n",
    "    a. Use inp_data_dime. Apply PCA and compute all the possible principle components (PCs). How many PCs are required to reproduce the 95% characteristics of original data. Plot it with appropriate diagram. Also print the top 5 eigen vectors (5 marks)\n",
    "\n",
    "    b. Create a random matrix (M) of size 20 x 8 and compute singular values, left singular matrix and right singular matrix using Singular Value Decomposition. Try to reproduce the M back using singular values and vectors. (5 Marks)\n",
    "\n",
    "    c. Apply SVD on inp_data_dime and compare the SVD transform data with PCA transformed data. Also compare the top 5 singular vectors with eigen vector. How many Singular vectors are required to reproduce the 95% charecteristics of original data. (5 Marks)\n",
    "\n",
    "    d. Clustering: Use PCA dimensions to cluster the data. Apply K-means and Agglomerative clustering. (5 Marks)\n",
    "\n",
    "## SECTION C (15 marks)\n",
    "### 3. Recommendation Systems \n",
    "    a. Build the popularity-based recommendation system and suggest top 5 items. (5 Marks)\n",
    "    \n",
    "    b. Build collaborative recommendation engine to recommend a top product/item to the specific user. Measure the model quality in terms of RMSE. (10 Marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb224a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscriminant_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearDiscriminantAnalysis\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNNWithMeans,SVDpp\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage , dendrogram, fcluster,cophenet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from surprise import KNNWithMeans,SVDpp\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split,cross_validate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9b725",
   "metadata": {},
   "source": [
    "## SECTION A \n",
    "### 1. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebadc49",
   "metadata": {},
   "source": [
    "### a. Read the dataset and perform required cleaning and preprocessing prior to model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4997630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = pd.read_csv(\"C:\\Datasets\\dataset3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e614b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the datasets\n",
    "data_dime = data.iloc[0:5891, 0:30]\n",
    "data_recom = data.iloc[:, 30:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9634a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data for Dimensionality Reduction:\")\n",
    "data_dime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f59c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData for Recommendation System:\")\n",
    "data_recom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d53b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_recom.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e1004",
   "metadata": {},
   "source": [
    "### b. Calculate five-point summary for numerical variables. Summarize observations for categorical variables – no. of categories, % observations in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five-point summary for numerical variables\n",
    "print(\"Five-point summary for numerical variables:\")\n",
    "data_dime.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed60c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize observations for categorical variables\n",
    "categorical_summary = data_dime.select_dtypes(include=['object']).apply(lambda x: x.value_counts(normalize=True) * 100)\n",
    "print(\"\\nSummary for categorical variables:\")\n",
    "categorical_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6e9d8",
   "metadata": {},
   "source": [
    "### c. Perform univariate and bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Univariate analysis\n",
    "for column in data_dime.select_dtypes(include=['number']).columns:\n",
    "    sns.histplot(data_dime[column], kde=True)\n",
    "    plt.title(f'Univariate Analysis of {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f8941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis\n",
    "sns.pairplot(data_dime.select_dtypes(include=['number']))\n",
    "plt.title('Bivariate Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e2dde",
   "metadata": {},
   "source": [
    "### d. Scale / Transform/ clean the data so that it is suitable for model building. Drop “SalePrice” before using clustering methods, as this is the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2810876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target attribute \"SalePrice\"\n",
    "df = data_dime.drop(columns=['SalePrice'])\n",
    "df_num = data_dime.select_dtypes(include=np.number)\n",
    "df_cat = data_dime.select_dtypes(include=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2086daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in df_cat.columns:\n",
    "    df_cat[i] = le.fit_transform(df_cat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([df_num,df_cat],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df1)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516846f7",
   "metadata": {},
   "source": [
    "## SECTION B (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87cb3c",
   "metadata": {},
   "source": [
    "### a. Apply PCA and compute all the possible principle components (PCs). How many PCs are required to reproduce the 95% characteristics of original data. Plot it with appropriate diagram. Also print the top 5 eigen vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d18a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "pca_scaled_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Number of components to capture 95% variance\n",
    "num_components = pca.n_components_\n",
    "print(f\"Number of components to capture 95% variance: {num_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute explained variance\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(explained_variance,marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0867f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 eigenvalues\n",
    "top_eigenvalues = pca.explained_variance_[:5]\n",
    "print(\"Top 5 Eigenvalues:\")\n",
    "print(top_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 eigenvectors\n",
    "top_eigenvectors = pca.components_[:5]\n",
    "print(\"\\nTop 5 Eigenvectors:\")\n",
    "print(top_eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e553a2",
   "metadata": {},
   "source": [
    "### b. Create a random matrix (M) of size 20 x 8 and compute singular values, left singular matrix and right singular matrix using Singular Value Decomposition. Try to reproduce the M back using singular values and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a random matrix\n",
    "M = np.random.rand(20, 8)\n",
    "\n",
    "# Compute SVD\n",
    "U, S, VT = np.linalg.svd(M)\n",
    "\n",
    "# Reconstruct M using SVD components\n",
    "# S should be expanded to a diagonal matrix of shape (8, 8)\n",
    "S_diag = np.diag(S)\n",
    "\n",
    "# Reconstruct the original matrix M\n",
    "M_reconstructed = U[:, :8] @ S_diag @ VT\n",
    "\n",
    "print(\"Original Matrix M:\")\n",
    "print(M)\n",
    "\n",
    "print(\"\\nReconstructed Matrix M:\")\n",
    "print(M_reconstructed)\n",
    "\n",
    "# Check if reconstruction is close to the original\n",
    "reconstruction_error = np.linalg.norm(M - M_reconstructed)\n",
    "print(f\"Reconstruction error: {reconstruction_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc270e92",
   "metadata": {},
   "source": [
    "### c. Apply SVD on inp_data_dime and compare the SVD transform data with PCA transformed data. Also compare the top 5 singular vectors with eigen vector. How many Singular vectors are required to reproduce the 95% charecteristics of original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Apply SVD\n",
    "svd = TruncatedSVD(n_components=num_components)\n",
    "data_dime_svd = svd.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7673d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top 5 singular vectors with eigen vectors\n",
    "top_5_singular_vectors = svd.components_[:5]\n",
    "print(\"Top 5 singular vectors:\")\n",
    "print(top_5_singular_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components to explain 95% variance in SVD\n",
    "svd_explained_variance = svd.explained_variance_ratio_\n",
    "num_svd_components = next(i for i, total_var in enumerate(svd_explained_variance.cumsum()) if total_var >= 0.95) + 1\n",
    "print(f\"Number of SVD components to explain 95% variance = {num_svd_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3ec59",
   "metadata": {},
   "source": [
    "### d. Clustering: Use PCA dimensions to cluster the data. Apply K-means and Agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "# Use PCA dimensions for clustering\n",
    "pca_dims = pca_scaled_data[:, :num_components]\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=5, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(pca_dims)\n",
    "print(\"K-means clustering labels:\")\n",
    "print(kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative clustering\n",
    "agglo = AgglomerativeClustering(n_clusters=5)\n",
    "agglo_labels = agglo.fit_predict(pca_dims)\n",
    "print(\"\\nAgglomerative clustering labels:\")\n",
    "print(agglo_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44157a",
   "metadata": {},
   "source": [
    "## SECTION C \n",
    "### 3. Recommendation Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699afcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_recom.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20872e95",
   "metadata": {},
   "source": [
    "### a. Build the popularity-based recommendation system and suggest top 5 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_recom.columns = ['UserID', 'ItemID', 'Rating','Timestamp']\n",
    "\n",
    "# Calculate the average rating and the count of ratings for each item\n",
    "item_popularity = data_recom.groupby('ItemID').agg({'Rating': ['mean', 'count']})\n",
    "item_popularity.columns = ['Average_rating', 'Rating_count']\n",
    "\n",
    "# Sort items by rating count and mean rating\n",
    "top_items = item_popularity.sort_values(by=['Rating_count', 'Average_rating'], ascending=False).head(5)\n",
    "\n",
    "print(\"Top 5 popular items:\")\n",
    "top_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f753eb8",
   "metadata": {},
   "source": [
    "### b. Build collaborative recommendation engine to recommend a top product/item to the specific user. Measure the model quality in terms of RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb346def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse\n",
    "\n",
    "# Prepare data for collaborative filtering\n",
    "reader = Reader(rating_scale=(data_recom['Rating'].min(), data_recom['Rating'].max()))\n",
    "data_cf = Dataset.load_from_df(data_recom[['UserID', 'ItemID', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa28acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data_cf, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58957333",
   "metadata": {},
   "source": [
    "### (i) Build a user-based collaborative filtering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, KNNBasic\n",
    "\n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "model = KNNBasic(sim_options=sim_options)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf98dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RMSE\n",
    "\n",
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b44077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend top N items for a specific user\n",
    "def get_top_n_recommendations(algo, user_id, n=1):\n",
    "    # Get a list of all item ids\n",
    "    all_items = data_recom['ItemID'].unique()\n",
    "    # Get a list of items already rated by the user\n",
    "    rated_items = data_recom[data_recom['UserID'] == user_id]['ItemID']\n",
    "    # Get a list of items not yet rated by the user\n",
    "    items_to_recommend = [item for item in all_items if item not in rated_items]\n",
    "    \n",
    "    # Predict the rating for each item not yet rated\n",
    "    predictions = [algo.predict(user_id, item_id) for item_id in items_to_recommend]\n",
    "    \n",
    "    # Sort the predictions by estimated rating\n",
    "    predictions.sort(key=lambda x: x.est, reverse=True)\n",
    "    \n",
    "    # Return the top N items\n",
    "    top_n_items = predictions[:n]\n",
    "    return top_n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_id = data_recom['UserID'].iloc[10]  # Replace with a specific user id\n",
    "top_recommendations = get_top_n_recommendations(model, user_id, n=2)\n",
    "\n",
    "print(f\"Top recommendation for user {user_id}:\")\n",
    "for recommendation in top_recommendations:\n",
    "    print(f\"Item {recommendation.iid} with predicted rating {recommendation.est}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d9a28",
   "metadata": {},
   "source": [
    "### (ii) Using SVD for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVD(n_factors=50, n_epochs=200)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9374fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend top N items for a specific user\n",
    "def get_top_n_recommendations(algo, user_id, n=1):\n",
    "    # Get a list of all item ids\n",
    "    all_items = data_recom['ItemID'].unique()\n",
    "    # Get a list of items already rated by the user\n",
    "    rated_items = data_recom[data_recom['UserID'] == user_id]['ItemID']\n",
    "    # Get a list of items not yet rated by the user\n",
    "    items_to_recommend = [item for item in all_items if item not in rated_items]\n",
    "    \n",
    "    # Predict the rating for each item not yet rated\n",
    "    predictions = [algo.predict(user_id, item_id) for item_id in items_to_recommend]\n",
    "    \n",
    "    # Sort the predictions by estimated rating\n",
    "    predictions.sort(key=lambda x: x.est, reverse=True)\n",
    "    \n",
    "    # Return the top N items\n",
    "    top_n_items = predictions[:n]\n",
    "    return top_n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e035955",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_recom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m user_id \u001b[38;5;241m=\u001b[39m data_recom[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUserID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Replace with a specific user id\u001b[39;00m\n\u001b[0;32m      3\u001b[0m top_recommendations \u001b[38;5;241m=\u001b[39m get_top_n_recommendations(model, user_id, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop recommendation for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_recom' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_id = data_recom['UserID'].iloc[2]  # Replace with a specific user id\n",
    "top_recommendations = get_top_n_recommendations(model, user_id, n=5)\n",
    "\n",
    "print(f\"Top recommendation for user {user_id}:\")\n",
    "for recommendation in top_recommendations:\n",
    "    print(f\"Item {recommendation.iid} with predicted rating {recommendation.est}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106ad5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
