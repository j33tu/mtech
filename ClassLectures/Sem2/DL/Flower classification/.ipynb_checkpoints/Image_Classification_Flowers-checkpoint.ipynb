{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x182yUaaGOIx"
   },
   "source": [
    "### Download Flowers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "USgEKN9GGf60"
   },
   "outputs": [],
   "source": [
    "#You can download the data manually as well instead of using 'wget'\n",
    "!wget http://download.tensorflow.org/example_images/flower_photos.tgz --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cCPVwdxpDvR",
    "outputId": "21960408-245a-4204-8334-f1df468b4575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 223456\n",
      "-rw-r--r-- 1 root root 228813984 Feb 10  2016 flower_photos.tgz\n",
      "drwxr-xr-x 1 root root      4096 Jun 20 18:46 sample_data\n"
     ]
    }
   ],
   "source": [
    "#Check if file is downloaded\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r9NeLYrOpIc0"
   },
   "outputs": [],
   "source": [
    "#Unzip the data\n",
    "!tar -xf flower_photos.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd2Zz_mYrU7N",
    "outputId": "6e8c690e-53ff-4f1a-8582-95299bcb258b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 616\n",
      "drwx------ 2 270850 5000  36864 Feb 10  2016 daisy\n",
      "drwx------ 2 270850 5000  45056 Feb 10  2016 dandelion\n",
      "-rw-r----- 1 270850 5000 418049 Feb  9  2016 LICENSE.txt\n",
      "drwx------ 2 270850 5000  36864 Feb 10  2016 roses\n",
      "drwx------ 2 270850 5000  36864 Feb 10  2016 sunflowers\n",
      "drwx------ 2 270850 5000  40960 Feb 10  2016 tulips\n"
     ]
    }
   ],
   "source": [
    "#Check how data is organized\n",
    "!ls -l flower_photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz0v83zDhs2y"
   },
   "source": [
    "### Build batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-da8Sz_BpnpI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dQQ7Oksaq7tI"
   },
   "outputs": [],
   "source": [
    "#Define some parameters\n",
    "img_size = 60\n",
    "img_depth = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBkyCTo1qWMy"
   },
   "source": [
    "Create an ImageDataGenerator object, it can also split data between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bkUsCc6zp7Kp"
   },
   "outputs": [],
   "source": [
    "#ImageDataGenerator declaration with 20% data as test (80% for training)\n",
    "img_generator= tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3BVmtXdrHyh"
   },
   "source": [
    "ImageDataGenerator can read images directory and also resize them if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkCVDrPOqDjE",
    "outputId": "9997d96b-120e-4a84-ea3a-54e1463a433b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2939 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Build training generator.\n",
    "train_generator = img_generator.flow_from_directory('flower_photos',\n",
    "                                                    batch_size=64,\n",
    "                                                    target_size=(img_size, img_size),\n",
    "                                                    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Z7_0KoJGRDM",
    "outputId": "a6a31fe2-0c75-44ad-fb3b-28b8cdcef668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 731 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Build test generator\n",
    "test_generator = img_generator.flow_from_directory('flower_photos',\n",
    "                                                   target_size=(img_size, img_size),\n",
    "                                                   subset='validation',\n",
    "                                                   batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "ZnOY195Pt7mn",
    "outputId": "0b9a6c0d-995a-4bf4-e850-25e152d46be2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.preprocessing.image.DirectoryIterator</b><br/>def __init__(directory, image_data_generator, target_size=(256, 256), color_mode=&#x27;rgb&#x27;, classes=None, class_mode=&#x27;categorical&#x27;, batch_size=32, shuffle=True, seed=None, data_format=None, save_to_dir=None, save_prefix=&#x27;&#x27;, save_format=&#x27;png&#x27;, follow_links=False, subset=None, interpolation=&#x27;nearest&#x27;, keep_aspect_ratio=False, dtype=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py</a>Iterator capable of reading images from a directory on disk.\n",
       "\n",
       "Deprecated: `tf.keras.preprocessing.image.DirectoryIterator` is not\n",
       "recommended for new code. Prefer loading images with\n",
       "`tf.keras.utils.image_dataset_from_directory` and transforming the output\n",
       "`tf.data.Dataset` with preprocessing layers. For more information, see the\n",
       "tutorials for [loading images](\n",
       "https://www.tensorflow.org/tutorials/load_data/images) and\n",
       "[augmenting images](\n",
       "https://www.tensorflow.org/tutorials/images/data_augmentation), as well as\n",
       "the [preprocessing layer guide](\n",
       "https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
       "\n",
       "Args:\n",
       "    directory: Path to the directory to read images from. Each subdirectory\n",
       "      in this directory will be considered to contain images from one class,\n",
       "      or alternatively you could specify class subdirectories via the\n",
       "      `classes` argument.\n",
       "    image_data_generator: Instance of `ImageDataGenerator` to use for random\n",
       "      transformations and normalization.\n",
       "    target_size: tuple of integers, dimensions to resize input images to.\n",
       "    color_mode: One of `&quot;rgb&quot;`, `&quot;rgba&quot;`, `&quot;grayscale&quot;`. Color mode to read\n",
       "      images.\n",
       "    classes: Optional list of strings, names of subdirectories containing\n",
       "      images from each class (e.g. `[&quot;dogs&quot;, &quot;cats&quot;]`). It will be computed\n",
       "      automatically if not set.\n",
       "    class_mode: Mode for yielding the targets:\n",
       "        - `&quot;binary&quot;`: binary targets (if there are only two classes),\n",
       "        - `&quot;categorical&quot;`: categorical targets,\n",
       "        - `&quot;sparse&quot;`: integer targets,\n",
       "        - `&quot;input&quot;`: targets are images identical to input images (mainly\n",
       "          used to work with autoencoders),\n",
       "        - `None`: no targets get yielded (only input images are yielded).\n",
       "    batch_size: Integer, size of a batch.\n",
       "    shuffle: Boolean, whether to shuffle the data between epochs.\n",
       "    seed: Random seed for data shuffling.\n",
       "    data_format: String, one of `channels_first`, `channels_last`.\n",
       "    save_to_dir: Optional directory where to save the pictures being\n",
       "      yielded, in a viewable format. This is useful for visualizing the\n",
       "      random transformations being applied, for debugging purposes.\n",
       "    save_prefix: String prefix to use for saving sample images (if\n",
       "      `save_to_dir` is set).\n",
       "    save_format: Format to use for saving sample images (if `save_to_dir` is\n",
       "      set).\n",
       "    subset: Subset of data (`&quot;training&quot;` or `&quot;validation&quot;`) if\n",
       "      validation_split is set in ImageDataGenerator.\n",
       "    interpolation: Interpolation method used to resample the image if the\n",
       "      target size is different from that of the loaded image. Supported\n",
       "      methods are &quot;nearest&quot;, &quot;bilinear&quot;, and &quot;bicubic&quot;. If PIL version 1.1.3\n",
       "      or newer is installed, &quot;lanczos&quot; is also supported. If PIL version\n",
       "      3.4.0 or newer is installed, &quot;box&quot; and &quot;hamming&quot; are also supported.\n",
       "      By default, &quot;nearest&quot; is used.\n",
       "    keep_aspect_ratio: Boolean, whether to resize images to a target size\n",
       "        without aspect ratio distortion. The image is cropped in the center\n",
       "        with target aspect ratio before resizing.\n",
       "    dtype: Dtype to use for generated arrays.</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 449);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "keras.src.preprocessing.image.DirectoryIterator"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCpXm9s4rjM1"
   },
   "source": [
    "ImageDataGenerator returns 64 images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aUhNi9Krrpq7"
   },
   "outputs": [],
   "source": [
    "#Lets check the features (images) and Labels (flower class) returned by ImageDataGenerator\n",
    "X, y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_QIPMTvsZZV",
    "outputId": "1bb186f1-aca1-488e-e5a3-5ac5c979860b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape (64, 60, 60, 3)\n",
      "Actual labels shape (64, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Input features shape', X.shape)\n",
    "print('Actual labels shape', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NUb1CcQx_Zo",
    "outputId": "974576b5-5c95-4197-a82e-503f62676bc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iJgvxvh32gt",
    "outputId": "881dfd71-6843-4fe6-995c-3ad95626ca56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.,  16.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,  27.,\n",
       "        28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,\n",
       "        39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,\n",
       "        50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,\n",
       "        61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,\n",
       "        72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,\n",
       "        83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,\n",
       "        94.,  95.,  96.,  97.,  98.,  99., 100., 101., 102., 103., 104.,\n",
       "       105., 106., 107., 108., 109., 110., 111., 112., 113., 114., 115.,\n",
       "       116., 117., 118., 119., 120., 121., 122., 123., 124., 125., 126.,\n",
       "       127., 128., 129., 130., 131., 132., 133., 134., 135., 136., 137.,\n",
       "       138., 139., 140., 141., 142., 143., 144., 145., 146., 147., 148.,\n",
       "       149., 150., 151., 152., 153., 154., 155., 156., 157., 158., 159.,\n",
       "       160., 161., 162., 163., 164., 165., 166., 167., 168., 169., 170.,\n",
       "       171., 172., 173., 174., 175., 176., 177., 178., 179., 180., 181.,\n",
       "       182., 183., 184., 185., 186., 187., 188., 189., 190., 191., 192.,\n",
       "       193., 194., 195., 196., 197., 198., 199., 200., 201., 202., 203.,\n",
       "       204., 205., 206., 207., 208., 209., 210., 211., 212., 213., 214.,\n",
       "       215., 216., 217., 218., 219., 220., 221., 222., 223., 224., 225.,\n",
       "       226., 227., 228., 229., 230., 231., 232., 233., 234., 235., 236.,\n",
       "       237., 238., 239., 240., 241., 242., 243., 244., 245., 246., 247.,\n",
       "       248., 249., 250., 252., 253., 255.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "es0UPokXxIKM",
    "outputId": "85cf0e9c-d379-4242-f5a2-63522552c794"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDV8kVGpOi_w"
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1DOwu8Bhs29"
   },
   "source": [
    "### Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tWQJ4SzZhs2-"
   },
   "outputs": [],
   "source": [
    "#Clear any previous model from memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#Initialize model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#normalize data\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=(img_size,img_size,3,)))\n",
    "\n",
    "#Add Conv Layer\n",
    "model.add(tf.keras.layers.Conv2D(32,\n",
    "                                 kernel_size=(3,3),\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVbP_q5hxxme",
    "outputId": "02efacc3-1050-46f0-b642-eb2704c10421"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 58, 58, 32) dtype=float32 (created by layer 'conv2d')>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8WhjR0Tsxr9Q"
   },
   "outputs": [],
   "source": [
    "#normalize data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Conv Layer\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "#normalize data\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#Add Max Pool layer\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Add Dense Layers after flattening the data\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "#Add Dropout\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "#Add Output Layer\n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GsuEXbofhs3D"
   },
   "outputs": [],
   "source": [
    "#Specify Loass and Optimizer\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCB11Qm44ynY",
    "outputId": "b4bdabda-922a-447d-fb0e-2cd2d1815fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (Batch  (None, 60, 60, 3)         12        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 58, 58, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 56, 56, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 28, 28, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               6422656   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6443089 (24.58 MB)\n",
      "Trainable params: 6442891 (24.58 MB)\n",
      "Non-trainable params: 198 (792.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v7UlgC5hs3g"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cklB73X8yCvt"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('flowers.h5',\n",
    "                                                      save_best_only=True,\n",
    "                                                      monitor='val_accuracy',\n",
    "                                                      mode='max',\n",
    "                                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldJt7GHB5dtQ",
    "outputId": "f347ce50-f1cb-4ad5-aeaf-83b0e143fbe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2939//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YGqTxKQhs3l",
    "outputId": "5104c8f2-8ad9-42bf-eb1e-a0ebdb8945ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4363 - accuracy: 0.3920\n",
      "Epoch 1: val_accuracy improved from -inf to 0.26136, saving model to flowers.h5\n",
      "45/45 [==============================] - 66s 1s/step - loss: 1.4363 - accuracy: 0.3920 - val_loss: 12.2734 - val_accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2762 - accuracy: 0.4946\n",
      "Epoch 2: val_accuracy did not improve from 0.26136\n",
      "45/45 [==============================] - 65s 1s/step - loss: 1.2762 - accuracy: 0.4946 - val_loss: 29.2677 - val_accuracy: 0.2216\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.1613 - accuracy: 0.5134\n",
      "Epoch 3: val_accuracy did not improve from 0.26136\n",
      "45/45 [==============================] - 63s 1s/step - loss: 1.1613 - accuracy: 0.5134 - val_loss: 33.7433 - val_accuracy: 0.2472\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0453 - accuracy: 0.5607\n",
      "Epoch 4: val_accuracy improved from 0.26136 to 0.28835, saving model to flowers.h5\n",
      "45/45 [==============================] - 63s 1s/step - loss: 1.0453 - accuracy: 0.5607 - val_loss: 21.9670 - val_accuracy: 0.2884\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0215 - accuracy: 0.5920\n",
      "Epoch 5: val_accuracy improved from 0.28835 to 0.40199, saving model to flowers.h5\n",
      "45/45 [==============================] - 62s 1s/step - loss: 1.0215 - accuracy: 0.5920 - val_loss: 11.6946 - val_accuracy: 0.4020\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9292 - accuracy: 0.6177\n",
      "Epoch 6: val_accuracy improved from 0.40199 to 0.47585, saving model to flowers.h5\n",
      "45/45 [==============================] - 61s 1s/step - loss: 0.9292 - accuracy: 0.6177 - val_loss: 6.1102 - val_accuracy: 0.4759\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.6536\n",
      "Epoch 7: val_accuracy improved from 0.47585 to 0.54545, saving model to flowers.h5\n",
      "45/45 [==============================] - 69s 2s/step - loss: 0.8286 - accuracy: 0.6536 - val_loss: 3.1627 - val_accuracy: 0.5455\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7911 - accuracy: 0.6810\n",
      "Epoch 8: val_accuracy did not improve from 0.54545\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.7911 - accuracy: 0.6810 - val_loss: 3.2150 - val_accuracy: 0.5384\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7260 - accuracy: 0.7071\n",
      "Epoch 9: val_accuracy improved from 0.54545 to 0.57955, saving model to flowers.h5\n",
      "45/45 [==============================] - 73s 2s/step - loss: 0.7260 - accuracy: 0.7071 - val_loss: 1.7085 - val_accuracy: 0.5795\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7125 - accuracy: 0.7061\n",
      "Epoch 10: val_accuracy did not improve from 0.57955\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.7125 - accuracy: 0.7061 - val_loss: 1.4975 - val_accuracy: 0.5312\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.7287\n",
      "Epoch 11: val_accuracy did not improve from 0.57955\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.6474 - accuracy: 0.7287 - val_loss: 1.4788 - val_accuracy: 0.5497\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5885 - accuracy: 0.7593\n",
      "Epoch 12: val_accuracy did not improve from 0.57955\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.5885 - accuracy: 0.7593 - val_loss: 1.7538 - val_accuracy: 0.5696\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5519 - accuracy: 0.7670\n",
      "Epoch 13: val_accuracy improved from 0.57955 to 0.60369, saving model to flowers.h5\n",
      "45/45 [==============================] - 63s 1s/step - loss: 0.5519 - accuracy: 0.7670 - val_loss: 1.5562 - val_accuracy: 0.6037\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5207 - accuracy: 0.7944\n",
      "Epoch 14: val_accuracy did not improve from 0.60369\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.5207 - accuracy: 0.7944 - val_loss: 1.7103 - val_accuracy: 0.5682\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.7850\n",
      "Epoch 15: val_accuracy did not improve from 0.60369\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.5155 - accuracy: 0.7850 - val_loss: 1.5568 - val_accuracy: 0.5852\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4495 - accuracy: 0.8066\n",
      "Epoch 16: val_accuracy improved from 0.60369 to 0.61222, saving model to flowers.h5\n",
      "45/45 [==============================] - 68s 2s/step - loss: 0.4495 - accuracy: 0.8066 - val_loss: 1.5145 - val_accuracy: 0.6122\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4011 - accuracy: 0.8264\n",
      "Epoch 17: val_accuracy did not improve from 0.61222\n",
      "45/45 [==============================] - 67s 1s/step - loss: 0.4011 - accuracy: 0.8264 - val_loss: 1.6823 - val_accuracy: 0.5923\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.8268\n",
      "Epoch 18: val_accuracy did not improve from 0.61222\n",
      "45/45 [==============================] - 63s 1s/step - loss: 0.4055 - accuracy: 0.8268 - val_loss: 1.8313 - val_accuracy: 0.6037\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.8508\n",
      "Epoch 19: val_accuracy did not improve from 0.61222\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.3674 - accuracy: 0.8508 - val_loss: 1.7547 - val_accuracy: 0.5994\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8518\n",
      "Epoch 20: val_accuracy did not improve from 0.61222\n",
      "45/45 [==============================] - 62s 1s/step - loss: 0.3523 - accuracy: 0.8518 - val_loss: 1.6903 - val_accuracy: 0.5923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ea238d070a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          epochs=20,\n",
    "          steps_per_epoch= 2939//64,  #Number of batches per epoch\n",
    "          validation_data=test_generator,\n",
    "          validation_steps = 731//64,\n",
    "          callbacks=[model_checkpoint]) #Number of test images//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9njmNCkdotB",
    "outputId": "ed8d48ef-91cb-45a9-efc4-61f5a82fcfa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 100, 100, 3)      12        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 98, 98, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 98, 98, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 98, 98, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 96, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 96, 96, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 147456)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               18874496  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,894,929\n",
      "Trainable params: 18,894,731\n",
      "Non-trainable params: 198\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcvtY4SDXm_y",
    "outputId": "fd1b8f27-47b3-4b97-fc18-f4498f583527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 60, 60, 3)        12        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 58, 58, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 56, 56, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 28, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               6422656   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,443,089\n",
      "Trainable params: 6,442,891\n",
      "Non-trainable params: 198\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
