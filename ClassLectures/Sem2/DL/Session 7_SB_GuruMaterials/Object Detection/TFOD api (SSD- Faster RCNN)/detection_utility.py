#!/usr/bin/env python
# coding: utf-8

# In[13]:


from __future__ import division
from __future__ import print_function
from __future__ import absolute_import
import subprocess
import os
import io
import pandas as pd
import tensorflow as tf
from absl import flags
import glob, pickle, json
import numpy as np
import argparse
import xml.etree.ElementTree as ET
from PIL import Image
from collections import namedtuple, OrderedDict
import cv2
from matplotlib import pyplot as plt


# In[2]:


def install_api():
    
    #check if Tensorflor object Detection API is installed
    a = subprocess.run('pip list  | grep object-detection', shell=True, capture_output=True)
    if str(a.stdout).find('object-detection') < 0:
        command = 'pip uninstall protobuf python3-protobuf;pip install --upgrade pip;pip install --upgrade protobuf'
        ret = subprocess.run(command, capture_output=True, shell=True)
        command = 'git clone https://github.com/tensorflow/models; pip install --upgrade pip; cd models/research; protoc object_detection/protos/*.proto --python_out=.;cp object_detection/packages/tf2/setup.py . ;python -m pip install .'
        print('Installing TensorFlow Object Detection API...this may take few minutes')
        ret = subprocess.run(command, capture_output=True, shell=True)
    else:
        print('TensorFlow Object Detection API found') 


# In[3]:


def xml_to_csv(path):
    """Iterates through all .xml files (generated by labelImg) in a given directory and 
    combines them in a single Pandas datagrame.

    Parameters:
    ----------
    path : {str}
        The path containing the .xml files
    Returns
    -------
    Pandas DataFrame
        The produced dataframe
    """

    xml_list = []
    for xml_file in glob.glob(path + '/*.xml'):
        tree = ET.parse(xml_file)
        root = tree.getroot()
        for member in root.findall('object'):
            try:
                value = (root.find('filename').text,
                        int(root.find('size')[0].text),
                        int(root.find('size')[1].text),
                        member[0].text,
                        int(member[4][0].text),
                        int(member[4][1].text),
                        int(member[4][2].text),
                        int(member[4][3].text)
                        )
                xml_list.append(value)
            except Exception as e:
                pass
                
    column_name = ['filename', 'width', 'height',
                'class', 'xmin', 'ymin', 'xmax', 'ymax']
    xml_df = pd.DataFrame(xml_list, columns=column_name)
    return xml_df


# In[4]:


def split(df, group):
    data = namedtuple('data', ['filename', 'object'])
    gb = df.groupby(group)
    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]

def create_tf_example(group, path):

    from object_detection.utils import dataset_util

    #with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:
    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:
        encoded_jpg = fid.read()
    encoded_jpg_io = io.BytesIO(encoded_jpg)
    image = Image.open(encoded_jpg_io)
    width, height = image.size

    filename = group.filename.encode('utf8')
    image_format = b'jpg'
    # check if the image format is matching with your images.
    xmins = []
    xmaxs = []
    ymins = []
    ymaxs = []
    classes_text = []
    classes = []

    for index, row in group.object.iterrows():
        xmins.append(row['xmin'] / width)
        xmaxs.append(row['xmax'] / width)
        ymins.append(row['ymin'] / height)
        ymaxs.append(row['ymax'] / height)
        classes_text.append(row['class'].encode('utf8'))
        #classes.append(class_text_to_int(row['class']))
        classes.append(row['label'])


    tf_example = tf.train.Example(features=tf.train.Features(feature={
        'image/height': dataset_util.int64_feature(height),
        'image/width': dataset_util.int64_feature(width),
        'image/filename': dataset_util.bytes_feature(filename),
        'image/source_id': dataset_util.bytes_feature(filename),
        'image/encoded': dataset_util.bytes_feature(encoded_jpg),
        'image/format': dataset_util.bytes_feature(image_format),
        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
        'image/object/class/label': dataset_util.int64_list_feature(classes),
    }))
    return tf_example


# In[8]:


def creat_label_map(label_class_dict, path=''):
    pbtxt_file_txt = ''
    for label in sorted(label_class_dict.keys()):
        
        pbtxt_file_txt += "item {\n  id: " + str(label) + "\n  name: '" +  label_class_dict[label] + "'\n}\n\n"

    with open(path+'label_map.txt','w') as pbfile:
        pbfile.write(pbtxt_file_txt)


# In[6]:


def split_data(df, train_size, path=''):
    
    
    all_files = df['filename'].unique()
    
    #Split images between training and test
    
    #80% of the data will be used for training
    mask = np.random.rand(all_files.shape[0]) < train_size

    #Get Training and Test images
    train_images = all_files[mask]
    test_images = all_files[~mask] 

    #Split dataframe between training and test
    train_df = df[df['filename'].isin(train_images)]
    test_df = df[df['filename'].isin(test_images)]
    train_df.to_csv(path + 'train.csv', index=False)
    test_df.to_csv(path + 'test.csv', index=False)


# In[7]:


def generate_tfrecord(csv_input, img_path, label_path, output_path):
    
    df = pd.read_csv(csv_input)
    writer = tf.io.TFRecordWriter(output_path)
    path = os.path.join(os.getcwd(), img_path)
    grouped = split(df, 'filename')
    for group in grouped:
        tf_example = create_tf_example(group, path)
        writer.write(tf_example.SerializeToString())

    writer.close()


# In[12]:


#Function to get predictions from a Detection model
def detector_prediction(model, image_file, label_class_dict,img_array=None, confidence_threshold=0.5):

    """
    image_file: File path of the image for which prediction needs to be done
    img_array: only considered if image_file is not specified
    confidence_threshold: Minimum confidence/probability for prediction to be considered
    """
    #Load image
    if(image_file):
        img = tf.keras.preprocessing.image.load_img(image_file)
        img_array = tf.keras.preprocessing.image.img_to_array(img).astype('uint8')
    
    #Make it a batch of one example
    img_array = tf.expand_dims(img_array, axis=0)

    #Prediction
    output = model(img_array) #get list of tensors discussed above as output
    
    #print(output)
    detection_classes = output['detection_classes'].numpy()[0]
    detection_scores = output['detection_scores'].numpy()[0] #get detection scores
    detection_boxes = output['detection_boxes'].numpy()[0]

    #Select predictions for which probability is higher than confidence_threshold
    selected_predictions = detection_scores >= confidence_threshold

    selected_prediction_scores = detection_scores[selected_predictions]
    selected_prediction_classes = detection_classes[selected_predictions]
    selected_prediction_boxes = detection_boxes[selected_predictions]

    #De-normalize box co-ordinates (multiply x-coordinates by image width and y-coords by image height)
    img_h, img_w = img_array.shape[1:3]

    for i in range(selected_prediction_boxes.shape[0]):
        
        selected_prediction_boxes[i,0] *= img_h #ymin * img_w
        selected_prediction_boxes[i,1] *= img_w #xmin * img_h
        selected_prediction_boxes[i,2] *= img_h #ymax * img_w
        selected_prediction_boxes[i,3] *= img_w #xmax * img_h

    #Make all co-ordinates as integer
    selected_prediction_boxes= selected_prediction_boxes.astype(int)

    #Convert class indexes to actual class labels
    predicted_classes = []
    for i in range(selected_prediction_classes.shape[0]):
        predicted_classes.append(label_class_dict[str(int(selected_prediction_classes[i]))])

    #Number of predictions
    selected_num_predictions = selected_prediction_boxes.shape[0]

    return {'Total Predictions': selected_num_predictions,
            'Classes': predicted_classes, 
            'Scores': selected_prediction_scores, 
            'Box coordinates': selected_prediction_boxes}


# In[16]:


def visualize_output(output, image_file, confidence_threshold=0.5):

    #Read image
    img = cv2.imread(image_file)

    #Draw rectangle for predicted boxes, also add predicted classes
    for i in range(output['Box coordinates'].shape[0]):

        box = output['Box coordinates'][i]
        
        #Draw rectangle - (ymin, xmin, ymax, xmax)
        img = cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)
        
        #Add Label - Class name and confidence level
        label = output['Classes'][i] + ': ' + str(round(output['Scores'][i],2))
        img = cv2.putText(img, label, (box[1], box[0]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)
    
    #Conver BGR image to RGB to use with Matplotlib
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    #Display image
    plt.figure(figsize=(10,6))
    plt.imshow(img)
    plt.show()


# In[17]:


def video_predict(model, label_class_dict, video_name=0, 
        confidence_threshold=0.5, save_path=None):

    """
    video_name: filepath of the video file on which you want to predict. Ignore 
    this parameter if want to test on webcam (will not work on Google colab)

    confidence_threshold: threhold that model should use to decide if a object
    is really there in the image 

    save_path: If you want to save the displayed results, provide the file path 
    for the generated mp4 file.
    """

    #Load video
    capture = cv2.VideoCapture(video_name)
    
    #Get the first frame
    hasFrame, frame = capture.read()

    img_h = frame.shape[0]
    img_w = frame.shape[1]
    
    #if save_path given, initialize video writer
    if save_path:
        _fourcc = cv2.VideoWriter_fourcc(*'VP90')
        _out = cv2.VideoWriter(save_path, _fourcc, 25, (img_w,img_h))
    
    num_frames = 0
    while hasFrame:
        
        try:
            #Convert frame to RGB color as opencv reads it in BGR format
            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            #Make it a batch of one example
            #img_array = tf.expand_dims(img, axis=0)

            #Call model prediction function above
            output = detector_prediction(model, None, label_class_dict, img_array=img, confidence_threshold=confidence_threshold)

            #Draw rectangle for predicted boxes, also add predicted classes
            for i in range(output['Box coordinates'].shape[0]):

                box = output['Box coordinates'][i]
                
                #Draw rectangle 
                frame = cv2.rectangle(frame, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)
        
                #Add Label - Class name and confidence level
                label = output['Classes'][i] + ': ' + str(round(output['Scores'][i],2))
                frame = cv2.putText(frame, label, (box[1], box[0]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, 
                                    (255,255,255), 2, cv2.LINE_AA)
    
            
            #Save frame to the output file
            if save_path:
                _out.write(frame)
            
            #This will work normally but not in Google colab
            if save_path is None:
                cv2.imshow('Video', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    cv2.destroyAllWindows()
                    break

            #Read the next frame
            hasFrame, frame = capture.read()
        except Exception as e:
            print(e)
            hasFrame, frame = capture.read()
        num_frames += 1
    if save_path is None:
        cv2.destroyAllWindows()
    capture.release()
    
    print('Number of frames processed', num_frames)
    #Close the output video file
    if save_path:
        _out.release()


# In[19]:


class DetectionModel():
    
    def __init__(self, model_url=None, img_path=None, label_path=None,
                 test_img_path=None, test_label_path=None, train_size=0.8, 
                 folder_path=''):
        
        self.model_url = model_url
        self.img_path = img_path
        self.test_img_path = test_img_path
        self.test_label_path = test_label_path
        self.train_size = train_size
        self.folder_path = folder_path
        self.detection_dir = 'detection_model'
        self.model = None
        if os.path.exists(folder_path + 'detection_model'):
            print('Found existing model')
            self.load_model()
            
        if self.model is not None:
            return

        #Install Tensorflow API
        install_api()
        
        
        #Preprare data
        df = xml_to_csv(label_path)
        
        #Data Encoding
        from sklearn import preprocessing
        le = preprocessing.LabelEncoder()
        df['label'] = le.fit_transform(df['class'])
        self.num_classes = len(df['class'].unique())
        #Object detection API expects index to start from 1 (and not 0)
        df['label'] = df['label'] + 1
        label_class_dict = dict(zip(df['label'], df['class']))
        
        #Save dict
        with open(folder_path + 'labels.json','w') as file:
            json.dump(label_class_dict, file)
        
        if test_label_path is None:
            #split data
            print('Found ', len(df['class'].unique()), 'classes and ', df.shape[0] , 'examples')
            split_data(df, train_size, path=folder_path)
        else:
            print('Found ', len(df['class'].unique()), 'classes and ', df.shape[0] , 'examples in training data')
            df.to_csv(folder_path + 'train.csv', index=False)
            test_df = xml_to_csv(test_label_path)
            print('Found ', len(test_df['class'].unique()), 'classes and ', test_df.shape[0] , 'examples in test data')
            test_df.to_csv(folder_path + 'test.csv', index=False)
            
        #Generate TF Records
        generate_tfrecord(folder_path + 'train.csv', img_path, label_path, folder_path + 'train.tfrecord')
        generate_tfrecord(folder_path + 'test.csv', img_path, label_path, folder_path + 'test.tfrecord')
        
        #Create Label Mapping File
        creat_label_map(label_class_dict, path=folder_path)
        
        #Download model
        model_file = model_url.split('/')[-1]
        command = 'wget -q ' + model_url + ';tar -xf ' + model_file + ';'
        a = subprocess.run(command, shell=True, capture_output=True)
        pretrained_model_dir = model_file.split('.')[0]
        command = 'cp ' + pretrained_model_dir + '/pipeline.config .'
        ret = subprocess.run(command, capture_output=True, shell=True)
        
        #Prepare config file
        self.config_file = ''
        
        #Copy model_main_tf2 and
        command = 'cp models/research/object_detection/model_main_tf2.py .'
        ret = subprocess.run(command, capture_output=True, shell=True)
        command = 'cp models/research/object_detection/exporter_main_v2.py .'
        ret = subprocess.run(command, capture_output=True, shell=True)
        
    def train(self, config_file, num_steps=100):
        
        self.config_file = config_file
        #Create training directory
        self.training_dir = 'training'
        command = 'mkdir ' + self.folder_path + self.training_dir
        ret = subprocess.run(command, capture_output=True, shell=True)
        
        #Fixing the CuDNN version issue - TEMPORARY
        #command = 'apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2'
        #start training
        print('Training in progress...')
        command = 'python model_main_tf2.py --model_dir='+ self.folder_path + self.training_dir
        command = command + ' --pipeline_config_path='+ self.config_file + ' --checkpoint_every_n=100 --alsologtostderr'
        ret = subprocess.run(command, capture_output=True, shell=True)
        
        #Export model
        command = 'python exporter_main_v2.py --input_type "image_tensor" --pipeline_config_path '
        command = command + self.config_file + ' --trained_checkpoint_dir ' + self.folder_path + self.training_dir + ' --output_directory ' + self.folder_path + 'detection_model'
        ret = subprocess.run(command, capture_output=True, shell=True)
        print('Completed training', num_steps, 'steps')
    
    def load_model(self):
        import tensorflow as tf
        
        saved_model_path = self.folder_path + self.detection_dir + '/saved_model'
        self.model = tf.saved_model.load(saved_model_path)

        with open(self.folder_path + 'labels.json', 'r') as file:
            self.label_class_dict = json.load(file)
        
    def predict(self, img_file, confidence_threshold=0.5, show=False):
        
        if self.model is None:
            self.load_model()
        
        results = detector_prediction(self.model, img_file, self.label_class_dict, confidence_threshold=confidence_threshold)
        
        if show:
            visualize_output(results, img_file)
        
        return results
    
    def process_video(self, videoname=0, confidence_threshold=0.5, output_dir=None):
        
        if self.model is None:
            self.load_model()
        video_predict(self.model, self.label_class_dict, video_name=videoname, 
                         confidence_threshold=confidence_threshold, 
                         save_path=output_dir
                        )


# In[ ]: